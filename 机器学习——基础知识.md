信息论相关的内容：
1、随机变量的熵：
    Entropy（x）=-∑p(x)logp(x)
    注： 
         （1）、Entropy（x）表示
         （2）、熵用来表示所有信息量的期望；
         （3）、信息熵越大，包含的信息就越多，那么随机变量的不确定性就越大。








